---
marp: true
theme: default
class: invert
paginate: true
backgroundColor: #1a1a1a
color: #e0e0e0
style: |
  section {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    padding: 60px 80px;
  }
  h1 {
    color: #4fc3f7;
    font-size: 2.2em;
    margin-bottom: 0.3em;
  }
  h2 {
    color: #81c784;
    font-size: 1.6em;
    margin-top: 0.5em;
    margin-bottom: 0.3em;
  }
  h3 {
    font-size: 1.2em;
    margin-top: 0.4em;
    margin-bottom: 0.3em;
  }
  p, ul, ol {
    font-size: 0.95em;
    line-height: 1.5;
    margin: 0.4em 0;
  }
  code {
    background: #2d2d2d;
    color: #f8f8f2;
    font-size: 0.9em;
  }
  pre {
    background: #2d2d2d;
    border-left: 4px solid #4fc3f7;
    padding: 0.8em;
    font-size: 0.8em;
    margin: 0.5em 0;
  }
  .columns {
    display: grid;
    grid-template-columns: repeat(2, 1fr);
    gap: 1rem;
  }
  .role-card {
    border: 2px solid #555;
    border-radius: 8px;
    padding: 1em;
    margin: 0.3em;
  }
  .manager { background: rgba(33, 150, 243, 0.15); }
  .developer { background: rgba(76, 175, 80, 0.15); }
  .release { background: rgba(255, 152, 0, 0.15); }
  .checklist { font-size: 0.85em; line-height: 1.6; }
  .diagram { font-family: monospace; font-size: 0.7em; line-height: 1.3; }
  .stats { font-size: 1.1em; font-weight: bold; color: #4fc3f7; }
  .emoji { font-size: 1.5em; }
---

<!--
Speaker Notes: I've been using Claude Code to work on the INAV drone autopilot project - it's a complex flight controller firmware with about 150,000 lines of code.

I ran into some problems working with Claude on this codebase, and this presentation is about how I solved those problems.
-->

# Context Engineering for Claude Code
## The INAV-Claude Project

**Sensei**
*How I got Claude to stop forgetting things*

---

<!--
Speaker Notes: Before we dive in, let me give you quick context on the project - but don't worry about the specifics, focus on the patterns.

INAV is an open-source drone autopilot firmware. 150,000 lines of code - C code for the firmware that runs on the flight controller, plus JavaScript for the desktop configuration tool.

It uses several different protocols - one's called MSP. There's a simulator for testing without hardware.

The specific domain doesn't matter - these patterns work for any large codebase.
-->

# Quick Context: INAV Project

**INAV:** Open-source drone autopilot firmware

**Codebase:** 150,000 lines
- C code (firmware)
- JavaScript (configurator)

It uses several different protocols, such as one called MSP.

*Focus on the patterns, not the specifics*

---

<!--
Speaker Notes: So here's the problem I kept running into.

I'm going to show you five principles that solved this and made Claude way more reliable. But first, let me show you what was going wrong.

When you're working on a big codebase with Claude Code, you run into these issues constantly.

Claude has a massive context window, but that's actually a problem - it can easily get distracted from the key instructions.

I'd write instructions for Claude to follow, but I kept seeing the same patterns. Claude would forget to check lock files. Skip testing. Use direct build commands instead of the agents.

Or worse - I'd paste in the entire coding standards document, and Claude would just gloss over the critical parts.

The problem isn't the AI being forgetful.

It's information overload combined with poor structure.

That's what context engineering solves.
-->

# The Problem

## Common Problems Without Context Engineering

<div class="checklist">

â˜ Forgot to check lock files
â˜ Skipped testing before PR
â˜ Didn't run code review
â˜ Missing commit message details
â˜ Loaded 100k lines but missed the critical 100

</div>

### âŒ Claude isn't bad - the process isn't structured!

### Root Cause: Information Architecture
âœ“ Not an AI limitation
âœ“ Structure problem

---

<!--
Speaker Notes: So why does this happen?

Think about what you're asking Claude to process.

You've got a hundred thousand lines of code. Five thousand lines of documentation. Build instructions, testing guides, project tracking, architecture docs.

It's like trying to find a specific sentence in a library by reading every book.

Sure, Claude has a huge context window - but when you load everything, the critical information gets buried.

When you need to know "check the lock file before starting," that detail is drowning in ten thousand lines of MSP protocol documentation you don't need right now.

Context engineering is just loading the right information at the right time.
-->

# Why This Happens

<div class="diagram">

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  100k Lines of Code                          â”‚
â”‚  + 5k Lines of Documentation                 â”‚
â”‚  + Build Instructions                        â”‚
â”‚  + Testing Guides                            â”‚
â”‚  + Project Tracking                          â”‚
â”‚  + Architecture Docs                         â”‚
â”‚  = 110k+ Lines Total                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Claude's Context  â”‚ â† Information overload
         â”‚    [saturated]       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Important info   â”‚
           â”‚   Make sure to a â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

</div>

**Critical details get lost in the noise**

---

<!--
Speaker Notes: So first solution - role separation.

Every conversation with Claude starts with "Which role should I take on today?"

And this isn't just organizational - it's fundamental to how context works.

The manager role loads guides about project tracking and task assignment. Maybe twelve hundred lines total.

The developer role loads coding standards and testing procedures. About twenty-five hundred lines.

Release manager loads build and release workflows. About a thousand lines.

Notice what each role DOESN'T load.

The developer never sees project management docs.

The manager never sees build instructions.

Without roles, you'd load all the documentation at once - that's 4,500 lines.

With roles, you only load what's relevant. The developer gets 2,500 lines. The manager gets 1,200.

Each role gets exactly what it needs, nothing more.

But that's just the beginning.
-->

# Solution 1 - Role Separation

<div class="columns">
<div>

<div class="role-card manager">

### ğŸ‘” MANAGER

**Context Size:** ~1,200 lines
**Focus:** Planning & Coordination
**Loads:** Project tracking, INDEX.md
**Doesn't Load:** Build instructions

</div>

<div class="role-card developer">

### ğŸ’» DEVELOPER

**Context Size:** ~2,500 lines
**Focus:** Implementation & Testing
**Loads:** Code guides, test docs
**Doesn't Load:** Project management

</div>

</div>
<div>

<div class="role-card release">

### ğŸ“¦ RELEASE MANAGER

**Context Size:** ~1,000 lines
**Focus:** Builds & Artifacts
**Loads:** Release workflow, changelogs
**Doesn't Load:** Implementation

</div>

<br/>

### Each role sees just what it needs

**WITHOUT roles:** All docs loaded = 4,500 lines
**WITH roles:** Only relevant docs = 2,500 lines max

<div style="margin-top: 1.5em; opacity: 0.3; font-size: 0.8em; overflow: hidden; max-height: 40px;">

### âš ï¸ Important Info Still Missing

Even with roles, we still need...

</div>

</div>
</div>

---

<!--
Speaker Notes: But roles alone aren't enough. Even with 2,500 lines, the developer still loads everything upfront. That's where the communication system comes in.

When the manager assigns a task, it creates a structured markdown file like this.

Notice what this does for context.

Everything the developer needs is in one file. The problem, success criteria, available resources, files to check.

The success criteria uses Claude's built-in TODO list format. Claude can track and update it as work progresses.

No searching through project docs.

No loading the entire issue tracker.

Just this one focused document.

And when the developer finishes? They write a completion report. What was done, what was tested, what PR was created.

This creates clear boundaries.

The developer never loads the manager's project tracking stuff.

The twelve-step process keeps information flowing to the right place.
-->

# Communication System

```markdown
# Task: Fix Terrain Data Not Loading

## Priority: HIGH

## Problem
User reports: "terrain data doesn't load" in Mission Control.

## Success Criteria              â† Clear goals
- [ ] Root cause identified
- [ ] Terrain data loads successfully
- [ ] PR created with tests

## Available Resources           â† What you have
- Chrome DevTools MCP available
- test-engineer agent for UI testing
```

**Everything in one focused file (80 lines)**
**No searching through project docs - no extra noise**

---

<!--
Speaker Notes: The developer follows these twelve steps. And context loads at specific points.

Step three - creating a branch? CRITICAL-BEFORE-CODE loads. "Check lock files, use inav-architecture before searching."

Steps four and seven - testing? CRITICAL-BEFORE-TEST loads testing philosophy.

Step eight - committing? Git best practices load.

Step nine - creating a PR? CRITICAL-BEFORE-PR loads and mandates testing and code review.

Each guide is about a hundred lines. Short, focused checklists.

They appear exactly when you need them.

No giant "how to do everything" document loaded upfront.

The guides even have self-improvement sections. When Claude discovers something important, it adds it to the guide for next time.
-->

# Solution 2 - 12-Step Workflow + JIT Docs

<div class="diagram" style="font-size: 0.65em;">

Developer 12-Step Process:          Documentation Loads:

1. Check inbox
2. Read task                        [Task file: 80 lines]

3. Create git branch         â”€â”€â”€â–º   [CRITICAL-BEFORE-CODE: 104 lines]
                                    â€¢ Check lock files
4. Reproduce bug (fails)     â”€â”€â”€â”   â€¢ Use agents, not direct commands
                                â”‚
5. Implement fix                â”‚   [CRITICAL-BEFORE-TEST: 113 lines]
                                â”œâ”€â–º â€¢ Test philosophy
6. Compile code                 â”‚   â€¢ Edge cases
                                â”‚
7. Verify fix (passes)       â”€â”€â”€â”˜

8. Commit changes            â”€â”€â”€â–º   [CRITICAL-BEFORE-COMMIT: 105 lines]
                                    â€¢ Git best practices

9. Create PR                 â”€â”€â”€â–º   [CRITICAL-BEFORE-PR: 171 lines]
                                    â€¢ MANDATORY testing
10. Check bot suggestions           â€¢ MANDATORY code review

11. Report completion
12. Archive assignment

</div>

**Total docs loaded: ~600 lines vs. 5,000+ all at once**

---

<!--
Speaker Notes: Looking back at the twelve steps, notice that several of them just say "call an agent."

Step six - compile? Call inav-builder.

Steps four and seven - testing? Call test-engineer.

Step nine - code review? Call inav-code-review.

That's where agents shine.

Agents in Claude Code are specialized subprocesses. Separate Claude instances with their own focused context. Think of them as expert consultants you call in for specific tasks.

Each agent has narrow knowledge. And critically, each agent has its own context window.

The agent's work doesn't pollute the main context.

The inav-builder? 282 lines representing about three thousand lines of build knowledge.

The test-engineer? 492 lines representing testing knowledge.

The msp-expert? 271 lines but it represents five thousand lines of protocol docs.

Ten agents total. Thirty-three hundred lines of definitions representing twenty-six thousand lines of knowledge.

What does that mean? Each agent definition is short - maybe 300 lines. But that definition gives it access to thousands of lines of documentation without loading it into the main context.

The main Claude session never loads that.

Agent spawns. Does its job. Returns the result. Disappears.

Focused context.
-->

# Specialized Agents

<div class="columns">
<div>

### ğŸ”¨ inav-builder

**Agent File:** 282 lines
**Represents:** ~3,000 lines

**Knows:**
â€¢ CMake build system
â€¢ ARM cross-compilation
â€¢ Linker compatibility

**Doesn't Know:**
â€¢ Mission planning
â€¢ MSP protocol

**Lifecycle:** Spawn â†’ Build â†’ Return

</div>
<div>

### ğŸ§ª test-engineer

**Agent File:** 492 lines
**Represents:** ~2,500 lines

**Knows:**
â€¢ Chrome DevTools Protocol
â€¢ UI testing strategies
â€¢ SITL simulator usage

**Doesn't Know:**
â€¢ Build systems
â€¢ Project management

**Lifecycle:** Spawn â†’ Test â†’ Report

</div>
</div>

<div style="text-align: center; margin-top: 1em;">

**10 agents total: 3,301 lines representing ~26,000 lines**
Main session never loads this - agents handle it.

</div>

---

<!--
Speaker Notes: So here's how it all fits together.

Skills are reusable workflows. The start-task skill handles all the setup automatically.

Roles define context boundaries. Developer loads different docs than manager.

Agents provide specialized knowledge on demand. Builder, tester, code reviewer.

Hooks enforce the rules. When Claude tries to run make directly, the hook intercepts and says "use the builder agent."

Each piece has a specific job.

Skills orchestrate multi-step stuff.

Roles load the right context.

Agents handle specialized tasks.

Hooks prevent mistakes.

Together they make Claude follow the right process without you constantly reminding it.

The structure does the work.
-->

<!--
Speaker Notes: Here's how it all fits together.
-->

<div class="diagram" style="font-size: 0.7em;">

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER: "Fix GPS bug"                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Developer Role       â”‚  [Focused Context]
      â”‚  Loads: 2,500 lines   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  /start-task skill     â”‚  [Reusable Workflow]
         â”‚  â€¢ Check locks         â”‚
         â”‚  â€¢ Create branch       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Execute 12-Step Workflow  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Spawn Agents as Needed  â”‚  [Narrow Expertise]
    â”‚  â€¢ inav-builder          â”‚
    â”‚  â€¢ test-engineer         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Hooks Enforce     â”‚  [Guardrails]
    â”‚  â€¢ Check commands  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

</div>

---

<!--
Speaker Notes: So hooks are the enforcement mechanism.

There's a PreToolUse hook written in Python. It intercepts every tool call before it executes.

When Claude tries to run make SITL directly, the hook catches it. Says "use the inav-builder agent". And it injects an explanation into Claude's context about why.

The hook also manages permissions using a YAML config file. Allow, deny, or ask about specific commands.

This prevents destructive operations and makes sure the right process gets followed.

Without hooks, Claude just forgets in the next session. You have to tell it the same thing again.

The SessionStart hook verifies role selection happened.

Hooks are like guardrails. They keep Claude from making the same mistakes.
-->

# Hooks Enforce

<div class="columns">
<div>

### WITHOUT HOOKS

Claude: [runs `make SITL` directly with improper flags]
**Result:** âŒ Build fails
â€¢ Cryptic errors

User: "Use the build script"
Claude: "Oh, sorry!"

*[Next task...]*

Claude: [runs `make SITL` again]
User: <span class="emoji">ğŸ˜¤</span>

</div>
<div>

### WITH HOOKS

Claude: [tries `make SITL`]

Hook: ğŸ›‘ **INTERCEPTED**
Hook: "Use inav-builder agent"

Claude: [uses inav-builder]
**Result:** âœ… Build succeeds

*[Next task...]*

Claude: [starts typing "make"]
Hook: ğŸ›‘ **INTERCEPTED**
Claude: [uses inav-builder]

User: <span class="emoji">ğŸ˜Š</span>

</div>
</div>

### Hooks enforce the rules automatically

---

<!--
Speaker Notes: Let me walk you through a real project.

User reports terrain data doesn't load.

Manager creates an eighty-line task assignment. Clear criteria.

Developer reads it. CRITICAL-BEFORE-CODE guide loads and says "use test-engineer for testing."

Developer spawns the test-engineer agent. It uses Chrome DevTools to investigate.

Agent finds the issue. A function was commented out during an ESM migration in December.

Developer implements the fix using Chart.js v4.

Runs inav-code-review agent to check quality.

Creates the PR.

Same-day completion.

Total context loaded across the entire workflow? About fifteen hundred lines.

Without this system, Claude would search around, load docs, pull in related files. Probably ten to fifteen thousand lines of mostly irrelevant stuff.

That's the difference.
-->

# Real Example

### Screenshots advancing every 10 seconds (1:40 total)

<div style="text-align: center; font-size: 0.8em;">

**0:00** â†’ Problem: Terrain chart not displaying
**0:10** â†’ Task assigned: 80-line structured file
**0:20** â†’ Developer starts, guide loads
**0:30** â†’ test-engineer investigates with DevTools
**0:40** â†’ Root cause: plotElevation() commented out
**0:50** â†’ Fix: Chart.js v4 integration
**1:00** â†’ inav-code-review checks quality
**1:10** â†’ Success: Chart displays correctly
**1:20** â†’ PR #2518 created
**1:30** â†’ Context comparison

</div>

### Context Loaded: ~1,500 lines
### Without System: ~10-15k lines of scattered docs

**Focused workflow wins**

---

<!--
Speaker Notes: So what did this achieve?

Seventy-eight projects completed in the last two months. Made me way more productive than the previous three INAV maintainers.

But the real wins? Consistency and efficiency.

Projects follow the twelve-step workflow consistently.

Testing and code review happen consistently. Not perfectly every single time, but way better than without the system.

Hooks catch most mistakes automatically.

Focused context means faster responses. Fewer mistakes. Better adherence to guidelines.

Claude follows a real development workflow, not just ad-hoc code generation.
-->

# Results from Real-World Use

<div class="columns">
<div>

### ğŸ“Š PRODUCTIVITY & CONSISTENCY

**Projects Completed:** <span class="stats">78 in last 2 months</span>

**Process Consistency:** <span class="stats">Really high</span>
âœ“ Projects follow 12-step workflow
âœ“ Testing before PR
âœ“ Code review before PR
âœ“ Lock file checks

**Much more productive than 3 previous maintainers**

</div>
<div>

### ğŸ“‰ CONTEXT EFFICIENCY

**Codebase:** 150,000 lines

**Typical task loads:**
â€¢ ~1,500 lines (with system)
â€¢ ~10-15k lines (without)

<br/>

**Result:**
â€¢ Faster responses
â€¢ Fewer mistakes
â€¢ Better guideline adherence
â€¢ Follows a real workflow

</div>
</div>

---

<!--
Speaker Notes: Five principles you can use for your own projects.

One - structure by role and phase. Different tasks need different context.

Two - load docs just-in-time, not all upfront. Timing matters as much as content.

Three - use specialized agents for focused tasks. Narrow context means better results.

Four - use hooks to enforce rules automatically. Automation beats repetition.

Five - create clear communication boundaries. Roles communicate but don't overlap.

These aren't specific to INAV. They work for any large codebase.

And you can adopt them incrementally.

Start with role separation. Add JIT guides. Build agents as you need them.

You don't need to build everything at once.

Start small and scale.
-->

# 5 Principles You Can Use

### âœ“ 1. STRUCTURE BY ROLE AND PHASE
   â†³ Manager/Developer/Release

### âœ“ 2. LOAD DOCS JUST-IN-TIME
   â†³ CRITICAL-BEFORE-* guides

### âœ“ 3. USE SPECIALIZED AGENTS
   â†³ inav-builder, test-engineer

### âœ“ 4. ENFORCE WITH HOOKS
   â†³ Intercepting 'make' commands

### âœ“ 5. CLEAR COMMUNICATION BOUNDARIES
   â†³ Task assignment files

<br/>

**These patterns work for any large codebase**

---

<!--
Speaker Notes: The system uses Claude Code to improve itself.

When we kept manually looking up MSP messages, I used the create-agent agent. It researched the protocol, designed a new msp-expert agent, wrote the agent file, and updated the docs.

The system actually improved itself.

Second, every guide has a lessons-learned section at the end.

When Claude discovers something important - like "always use inav-architecture before Grep" or "lock files need timestamps for debugging" - it adds that to the guide.

Future sessions benefit automatically.

It documents what it learns along the way.
-->

# Self-Improvement

<div class="columns">
<div>

### Creates Its Own Tools

**create-agent** builds new agents!

User: "We keep looking up MSP messages"

Claude:
1. Researches MSP docs
2. Designs msp-expert agent
3. Writes agent (271 lines)
4. Updates README

Yes, that sounds recursive - I'm using an agent to create another agent. But that's the power of the system: create-agent is itself a specialist that knows how to research documentation, design agent interfaces, and write good agents. It's using Claude to build better Claude tools.

**The project improved itself**

</div>
<div>

### Lessons Learned

Guides have self-documentation:

```markdown
### Lessons Learned

- **Lock file format**: Include
  timestamp for debugging

- **inav-architecture first**:
  Use before Grep - saves 10min

- **SITL directory**: Use
  build_sitl/ not build/
```

**It documents what it learns**

</div>
</div>

---

<!--
Speaker Notes: You can clone this and adapt it for your project.

The twelve-step workflow? That's universal for most development projects. Same steps whether you're working on firmware, web apps, data science, or infrastructure.

You can reuse the structure.

Just customize the content.

Clone the directories. Update the guides with your build commands and test frameworks. Create agents for your specific stuff.

The cool part? Use the create-agent agent to help build your new agents.

It'll research your docs and write good agents for you.

And the lessons-learned sections mean the system improves as you use it.

The same concept even applies beyond coding. Developing this presentation followed a similar process with feedback loops and iterative refinement.

These patterns have worked well.

You can adapt them to your codebase.
-->

# Adapt for Your Project

<div class="columns">
<div>

### ğŸ¯ UNIVERSAL WORKFLOW

**12 steps work for most development projects**
Firmware, web apps, data science...

**The STRUCTURE is reusable**
âœ“ Role separation
âœ“ JIT documentation
âœ“ Agent pattern
âœ“ Hook enforcement

**The CONTENT needs customization**
âœ— Your build commands
âœ— Your test frameworks
âœ— Your specific agents

</div>
<div>

### ğŸ“‹ GETTING STARTED

**Phase 1:** Role separation

**Phase 2:** JIT guides

**Phase 3:** First agent

**Phase 4:** Add hooks

*Can be done incrementally over weeks or months*

<br/>

**Examples:**
Python/Django, React/TypeScript, Rust

</div>
</div>

### Clone `.claude/` and `claude/`, then customize for your project

---

<!--
Speaker Notes: Thank you for your time!

I'm happy to answer questions.

One thing I'm curious about - would people find it useful to have a separate generic template repository?

Right now this is all in the INAV repo, which works great as a complete example.

But I'm wondering if there's interest in a clean "claude-code-template" repository with just the structure and patterns, that you could clone and customize for your own projects.

Show of hands - how many people are thinking "I'd like to adapt this for my project"?

And of those, would you prefer to fork the INAV repo and customize it, or would you want a clean template repo to start from?

I've analyzed the options in detail - there are pros and cons to each approach.

If there's enough interest, I can extract the generic structure into a separate repo.

[Pause for responses/discussion]

Feel free to reach out on GitHub if you want to discuss adaptation approaches or have questions about implementing this for your project.
-->

# Thank You!

## Next Steps

**Want to try this?**
1. Clone github.com/sensei-hacker/inav-claude
2. Read claude/README.md
3. Check claude/examples/ for templates
4. Open an issue if you need help

## Questions?

<div style="text-align: center; margin-top: 2em;">

**Repository:** github.com/sensei-hacker/inav-claude
**Contact:** sensei-hacker on GitHub

<br/>

*This is how you make Claude actually follow your process*

</div>

---

# Backup: File Structure

<div class="diagram" style="font-size: 0.6em;">

inavflight/
â”œâ”€â”€ .claude/                    # Claude Code configuration
â”‚   â”œâ”€â”€ settings.json           # Hooks, permissions
â”‚   â”œâ”€â”€ agents/                 # 10 agents, 3,301 lines
â”‚   â”‚   â”œâ”€â”€ inav-builder.md
â”‚   â”‚   â”œâ”€â”€ test-engineer.md
â”‚   â”‚   â””â”€â”€ msp-expert.md
â”‚   â”œâ”€â”€ skills/                 # 31 reusable workflows
â”‚   â””â”€â”€ hooks/                  # Enforcement scripts
â”‚
â”œâ”€â”€ claude/                     # Role-specific workspaces
â”‚   â”œâ”€â”€ manager/
â”‚   â”‚   â””â”€â”€ README.md           # 1,200 lines of context
â”‚   â”œâ”€â”€ developer/
â”‚   â”‚   â”œâ”€â”€ README.md           # 2,500 lines of context
â”‚   â”‚   â””â”€â”€ guides/
â”‚   â”‚       â”œâ”€â”€ CRITICAL-BEFORE-CODE.md      (104 lines)
â”‚   â”‚       â”œâ”€â”€ CRITICAL-BEFORE-TEST.md      (113 lines)
â”‚   â”‚       â”œâ”€â”€ CRITICAL-BEFORE-COMMIT.md    (105 lines)
â”‚   â”‚       â””â”€â”€ CRITICAL-BEFORE-PR.md        (171 lines)
â”‚   â”œâ”€â”€ projects/
â”‚   â”‚   â”œâ”€â”€ INDEX.md            # Active projects
â”‚   â”‚   â””â”€â”€ completed/          # 78 completed
â”‚   â””â”€â”€ locks/                  # Concurrency control
â”‚
â””â”€â”€ CLAUDE.md                   # Entry point (role selection)

</div>

---

# Backup: Limitations & Tradeoffs

<div class="columns">
<div>

### Setup Cost
- Initial structure takes time to build
- Need to maintain documentation

### Not Perfect
- Still requires some human oversight
- Context engineering isn't magic

</div>
<div>

### Best For
- Large codebases (50k+ lines)
- Repeated workflows
- Multiple contributors

### Overkill For
- Small scripts
- One-off projects
- Simple codebases

</div>
</div>
